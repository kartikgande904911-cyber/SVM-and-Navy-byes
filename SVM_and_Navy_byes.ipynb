{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1 : What is Information Gain, and how is it used in Decision Trees?\n",
        "==>Information gain is the reduction in uncertainty (entropy) after a dataset is split by a feature, and it measures how much information a feature provides about the class of an item.\n",
        "\n",
        "Usage in Decision Trees:\n",
        "1.Splitting criterion: Information gain serves as the splitting criterion for building the tree. At each node, the algorithm calculates the information gain for every available feature.\n",
        "2.Node splitting: The feature that yields the highest information gain is selected to create a split. This ensures that the resulting child nodes are as \"pure\" or homogeneous as possible.\n",
        "3.Recursive process: The process is repeated recursively for each child node. The information gain is calculated for all features again, and the best one is chosen to split that child node, continuing until a stopping criterion is met.\n",
        "4.Maximizing gain: The goal is to maximize the information gain at each step, which is equivalent to minimizing the weighted entropy of the child nodes."
      ],
      "metadata": {
        "id": "572imPK4R_PO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: What is the difference between Gini Impurity and Entropy?\n",
        "==>Entropy: In information theory, entropy quantifies the uncertainty or disorder in a dataset. It measures the amount of information needed to describe the class of an instance. The formula for entropy is:\n",
        "Entropy=-∑pilog2pi\n",
        "         i\n",
        "\n",
        "Gini Impurity: Gini impurity measures the probability of misclassifying a randomly chosen element from the dataset. It is calculated using the formula:\n",
        "[text{Gini} = 1 - \\sum_{i} p_i\\^2 \\]\n",
        "\n",
        "Key differences:\n",
        "1.Entropy:\n",
        "a)Slightly slower due to logarithmic calculations.\n",
        "b)Produces more balanced node partitions.\n",
        "c)More sensitive to subtle probability differences.\n",
        "d)Preferred when theoretical information gain matters.\n",
        "\n",
        "2.Gini Impurity:\n",
        "a)Faster computation since it avoids log operations.\n",
        "b)Creates splits quickly, favoring dominant classes.\n",
        "c)Less sensitive to small probability changes.\n",
        "d)Often default in libraries like CART.\n"
      ],
      "metadata": {
        "id": "ev-xjH0GR_Rs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3:What is Pre-Pruning in Decision Trees?\n",
        "==>Pre-pruning, also known as early stopping, is a technique used in decision tree algorithms to halt the growth of the tree before it becomes overly complex. This approach aims to prevent overfitting by stopping the tree's expansion based on predefined conditions, ensuring better generalization to unseen data.\n",
        "\n",
        "Key Techniques in Pre-Pruning:\n",
        "\n",
        "1.Maximum Depth: Limits the depth of the tree to a specified maximum level, preventing it from growing too deep.\n",
        "\n",
        "2.Minimum Samples per Leaf: Sets a minimum threshold for the number of samples required in each leaf node.\n",
        "\n",
        "3.Minimum Samples per Split: Specifies the minimum number of samples needed to split a node.\n",
        "\n",
        "4.Maximum Features: Restricts the number of features considered for splitting at each node.\n",
        "\n",
        "By applying these constraints, pre-pruning results in a simpler and more interpretable tree that is less likely to overfit the training data. It is particularly effective for large datasets where computational efficiency and model simplicity are critical."
      ],
      "metadata": {
        "id": "Wnq_u61_R_Ze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 4:Write a Python program to train a Decision Tree Classifier using Gini\n",
        "#Impurity as the criterion and print the feature importances (practical).\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Target labels\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the Decision Tree Classifier with Gini Impurity as the criterion\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "\n",
        "# Train the classifier on the training data\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Print the feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for feature, importance in zip(iris.feature_names, clf.feature_importances_):\n",
        "    print(f\"{feature}: {importance:.4f}\")\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "accuracy = clf.score(X_test, y_test)\n",
        "print(f\"\\nModel Accuracy on Test Set: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0GE92n3Mm60",
        "outputId": "80cca1a5-91b9-4885-f89d-4ccd37b2d77c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances:\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0191\n",
            "petal length (cm): 0.8933\n",
            "petal width (cm): 0.0876\n",
            "\n",
            "Model Accuracy on Test Set: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "X_2c3RoDR_fs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: What is a Support Vector Machine (SVM)?\n",
        "==>A support vector machine (SVM) is a type of supervised learning algorithm used in machine learning to solve classification and regression tasks. SVMs are particularly good at solving binary classification problems, which require classifying the elements of a data set into two groups.\n",
        "\n",
        "SVMs aim to find the best possible line, or decision boundary, that separates the data points of different data classes. This boundary is called a hyperplane when working in high-dimensional feature spaces. The idea is to maximize the margin, which is the distance between the hyperplane and the closest data points of each category, thus making it easy to distinguish data classes.\n",
        "\n",
        "SVMs are useful for analyzing complex data that a simple straight line can't separate. Called nonlinear SVMs, they do this by using a mathematical trick that transforms data into higher-dimensional space, where it is easier to find a boundary."
      ],
      "metadata": {
        "id": "gVnbOqmOR_ic"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: What is the Kernel Trick in SVM?\n",
        "==>The concept of the kernel trick is a cornerstone in the field of machine learning, particularly within the realm of support vector machines (SVMs). It's a clever mathematical technique that allows SVMs to operate in a higher-dimensional space without explicitly computing the coordinates of the data in that space. This is not just a computational convenience but a profound insight into the nature of learning algorithms and their interaction with data. The kernel trick hinges on the idea that by mapping data into a higher-dimensional feature space, one can transform nonlinearly separable data into a linearly separable format, thereby enabling the use of linear classifiers like SVMs on complex problems."
      ],
      "metadata": {
        "id": "mygeCS5bR_kY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 7: Write a Python program to train two SVM classifiers with Linear and RBF\n",
        "#kernels on the Wine dataset, then compare their accuracies.\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Wine dataset\n",
        "wine = datasets.load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# 2. Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 3. Train an SVM classifier with a Linear kernel\n",
        "svm_linear = SVC(kernel='linear', random_state=42)\n",
        "svm_linear.fit(X_train, y_train)\n",
        "\n",
        "# 4. Train an SVM classifier with an RBF kernel\n",
        "svm_rbf = SVC(kernel='rbf', random_state=42)\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "\n",
        "# 5. Make predictions on the test set\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "\n",
        "# 6. Calculate and compare the accuracies\n",
        "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
        "accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "print(f\"Accuracy of SVM with Linear kernel: {accuracy_linear:.4f}\")\n",
        "print(f\"Accuracy of SVM with RBF kernel: {accuracy_rbf:.4f}\")\n",
        "\n",
        "# Optional: Determine which kernel performed better\n",
        "if accuracy_linear > accuracy_rbf:\n",
        "    print(\"The Linear kernel performed better.\")\n",
        "elif accuracy_rbf > accuracy_linear:\n",
        "    print(\"The RBF kernel performed better.\")\n",
        "else:\n",
        "    print(\"Both kernels performed equally well.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_VrrIY5RPBIm",
        "outputId": "efafa787-17e3-463c-a358-82e0c6629a20"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of SVM with Linear kernel: 0.9815\n",
            "Accuracy of SVM with RBF kernel: 0.7593\n",
            "The Linear kernel performed better.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
        "==>Naive Bayes is a simple technique for constructing classifiers: models that assign class labels to problem instances, represented as vectors of feature values, where the class labels are drawn from some finite set. There is not a single algorithm for training such classifiers, but a family of algorithms based on a common principle: all naive Bayes classifiers assume that the value of a particular feature is independent of the value of any other feature, given the class variable. For example, a fruit may be considered to be an apple if it is red, round, and about 10 cm in diameter. A naive Bayes classifier considers each of these features to contribute independently to the probability that this fruit is an apple, regardless of any possible correlations between the color, roundness, and diameter features.\n",
        "\n",
        "why is it called \"Naïve\"?\n",
        "In this article, we’ll break down the reasoning behind the name, explain the assumptions that make it “naive,” and explore how it works. We’ll also highlight when and why this simple algorithm remains relevant in modern data science."
      ],
      "metadata": {
        "id": "HUP9CFIGR_mU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes.\n",
        "==>Gaussian Naive Bayes\n",
        "1.Gaussian Naive Bayes is useful when working with continuous values which probabilities can be modeled using a Gaussian distribution.\n",
        "2.Classification of continuous data: It can be used for tasks where features are numerical, such as classifying an email as spam or not based on word frequencies (though Multinomial NB is more common for this) or classifying a person as male or female based on height and weight.\n",
        "\n",
        "Multinomial naive Bayes\n",
        "1.A multinomial distribution is useful to model feature vectors where each value represents, for example, the number of occurrences of a term or its relative frequency. If the feature vectors have n elements and each of them can assume k different values with probability pk.\n",
        "2.Spam filtering: Classifying emails as \"spam\" or \"ham\" based on the words they contain.\n",
        "\n",
        "Bernoulli naive Bayes\n",
        "1.If X is random variable Bernoulli-distributed, it can assume only two values (for simplicity, let’s call them 0 and 1).\n",
        "2.Medical Diagnosis: Predicting the presence or absence of a disease based on binary symptoms (e.g., fever: yes/no, cough: yes/no)."
      ],
      "metadata": {
        "id": "JUFebxcJR_pi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Question 10: Breast Cancer Dataset\n",
        "#Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer\n",
        "#dataset and evaluate accuracy.\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "gnb = GaussianNB()\n",
        "\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Number of mislabeled points out of a total {X_test.shape[0]} test points : {(y_test != y_pred).sum()}\")\n",
        "print(f\"Target names: {data.target_names}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFQcfQVWSUz8",
        "outputId": "1dae23b8-7d84-4a8c-8889-fff7e9989b93"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9737\n",
            "Number of mislabeled points out of a total 114 test points : 3\n",
            "Target names: ['malignant' 'benign']\n"
          ]
        }
      ]
    }
  ]
}